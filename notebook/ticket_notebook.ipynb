{
 "cells": [
  {
   "cell_type": "code",
   "id": "f811cc1c-0a84-464d-8b46-2b45082c52bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:32:31.778016Z",
     "start_time": "2025-02-05T11:32:31.722700Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Imposta PROJECT_ROOT come la cartella principale (astromark-ai), un livello sopra \"notebook\"\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(\"[INFO] Project root aggiunto a sys.path:\", PROJECT_ROOT)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Project root aggiunto a sys.path: /Users/giuliosacrestano/PycharmProjects/astromark-ai\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "e3f75e39-9eb8-4a3a-a44c-2d0b1f6e2e2a",
   "metadata": {},
   "source": [
    "# Notebook Astromark AI\n",
    "\n",
    "Questo notebook carica i dati dei ticket, li unisce e li preprocessa (utilizzando spaCy per tokenizzazione, lemmatizzazione e NER), costruisce la pipeline di ML, esegue la grid search, salva/carica il modello e infine fornisce un esempio di utilizzo del ticket service."
   ]
  },
  {
   "cell_type": "code",
   "id": "e5ad5d70-73c4-4ae7-9381-53ef1a9a9bd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:32:37.266669Z",
     "start_time": "2025-02-05T11:32:35.767451Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from enum import Enum\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Imposta i percorsi relativi al project root\n",
    "BASE_DIR = PROJECT_ROOT\n",
    "DATA_RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "TRAINED_DIR = os.path.join(BASE_DIR, 'data', 'trained')\n",
    "\n",
    "TICKET_O3_PATH = os.path.join(DATA_RAW_DIR, 'ticket-o3.csv')\n",
    "TICKET_GEMINI_PATH = os.path.join(DATA_RAW_DIR, 'ticket-gemini-claude.csv')\n",
    "PROCESSED_DATA_PATH = os.path.join(PROCESSED_DIR, 'X_processed.csv')\n",
    "\n",
    "os.makedirs(DATA_RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(TRAINED_DIR, exist_ok=True)\n",
    "\n",
    "print('[INFO] Percorso per ticket-o3:', TICKET_O3_PATH)\n",
    "print('[INFO] Percorso per ticket-gemini:', TICKET_GEMINI_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Percorso per ticket-o3: /Users/giuliosacrestano/PycharmProjects/astromark-ai/data/raw/ticket-o3.csv\n",
      "[INFO] Percorso per ticket-gemini: /Users/giuliosacrestano/PycharmProjects/astromark-ai/data/raw/ticket-gemini-claude.csv\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "6149d1dc-fbd9-4248-9c38-9c8839ab1a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:32:49.380146Z",
     "start_time": "2025-02-05T11:32:49.348485Z"
    }
   },
   "source": [
    "# Caricamento e unione dei dati\n",
    "print('[INFO] Caricamento di ticket-o3...')\n",
    "dataframe_o3 = pd.read_csv(TICKET_O3_PATH, usecols=['titolo', 'messaggio', 'categoria'])\n",
    "print(f'[INFO] ticket-o3.csv caricato con shape: {dataframe_o3.shape}')\n",
    "\n",
    "print('[INFO] Caricamento di ticket-gemini-claude...')\n",
    "dataframe_gc = pd.read_csv(TICKET_GEMINI_PATH, usecols=['titolo', 'messaggio', 'categoria'])\n",
    "print(f'[INFO] ticket-gemini-claude.csv caricato con shape: {dataframe_gc.shape}')\n",
    "\n",
    "def merge_dataframes(frame1, frame2):\n",
    "    print('[INFO] Unione dei dataframe...')\n",
    "    frame = pd.concat([frame1, frame2])\n",
    "    frame['titolo_messaggio'] = frame['titolo'] + ' ' + frame['messaggio']\n",
    "    return frame[['titolo_messaggio', 'categoria']]\n",
    "\n",
    "merged_df = merge_dataframes(dataframe_o3, dataframe_gc)\n",
    "X = merged_df['titolo_messaggio']\n",
    "y = merged_df['categoria']\n",
    "print(f'[INFO] Dataset unito finale: {merged_df.shape}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Caricamento di ticket-o3...\n",
      "[INFO] ticket-o3.csv caricato con shape: (4027, 3)\n",
      "[INFO] Caricamento di ticket-gemini-claude...\n",
      "[INFO] ticket-gemini-claude.csv caricato con shape: (866, 3)\n",
      "[INFO] Unione dei dataframe...\n",
      "[INFO] Dataset unito finale: (4893, 2)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "a8a43f1d-9f64-4b75-9a32-dc264b4a607b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:33:00.187133Z",
     "start_time": "2025-02-05T11:32:59.887160Z"
    }
   },
   "source": [
    "# Preprocessing minimo del testo con spaCy\n",
    "print('[INFO] Caricamento del modello spaCy (it_core_news_sm)...')\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "# Definiamo i saluti da rimuovere\n",
    "GREETINGS_PATTERNS = [\n",
    "    r'\\bciao\\b', r'\\bbuongiorno\\b', r'\\bsalve\\b',\n",
    "    r'\\bbuonasera\\b', r'\\bbuon pomeriggio\\b', r'\\barrivederci\\b',\n",
    "    r'\\bbuonanotte\\b', r'\\ba presto\\b', r'\\baddio\\b', r'\\bsaluti\\b'\n",
    "]\n",
    "\n",
    "def remove_greetings(text):\n",
    "    pattern = re.compile('|'.join(GREETINGS_PATTERNS), flags=re.IGNORECASE)\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "def minimal_preprocess(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # rimozione URL\n",
    "    text = remove_greetings(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # rimozione punteggiatura\n",
    "    text = re.sub(r'\\d+', '', text)  # rimozione numeri\n",
    "    text = re.sub(r'\\s+', ' ', text)  # normalizzazione spazi\n",
    "    return text.strip()\n",
    "\n",
    "def process_text(text):\n",
    "    cleaned_text = minimal_preprocess(text)\n",
    "    doc = nlp(cleaned_text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct or token.is_space:\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        if lemma:\n",
    "            lemma = lemma.strip()\n",
    "            if lemma:\n",
    "                tokens.append(lemma)\n",
    "    for ent in doc.ents:\n",
    "        tokens.append(f\"NER_{ent.label_}\")\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def parallel_process_texts(series, n_jobs=-1):\n",
    "    print('[INFO] Preprocessing in parallelo con backend threading...')\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        processed = Parallel()(delayed(process_text)(text) for text in series)\n",
    "    return pd.Series(processed, index=series.index)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Caricamento del modello spaCy (it_core_news_sm)...\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "4b4e05fa-5b4f-4a5c-8b93-3ea32029ac0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:33:33.954146Z",
     "start_time": "2025-02-05T11:33:16.721639Z"
    }
   },
   "source": [
    "# Carica o processa i dati preprocessati\n",
    "if os.path.exists(PROCESSED_DATA_PATH):\n",
    "    print(f\"[INFO] Caricamento dei dati preprocessati da '{PROCESSED_DATA_PATH}'...\")\n",
    "    df = pd.read_csv(PROCESSED_DATA_PATH)\n",
    "    X_processed = df[\"processed_text\"]\n",
    "else:\n",
    "    print('[INFO] Dati preprocessati non trovati, inizio preprocessing in parallelo...')\n",
    "    X_processed = parallel_process_texts(X, n_jobs=-1)\n",
    "    print('[INFO] Salvataggio dei dati preprocessati in:', PROCESSED_DATA_PATH)\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=[\"processed_text\"])\n",
    "    X_processed_df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "    print('[INFO] Preprocessing completato e memorizzato.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dati preprocessati non trovati, inizio preprocessing in parallelo...\n",
      "[INFO] Preprocessing in parallelo con backend threading...\n",
      "[INFO] Salvataggio dei dati preprocessati in: /Users/giuliosacrestano/PycharmProjects/astromark-ai/data/processed/X_processed.csv\n",
      "[INFO] Preprocessing completato e memorizzato.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "f7a74a96-8db9-4f6a-8f1a-f35aace7ce3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:33:37.864311Z",
     "start_time": "2025-02-05T11:33:37.468439Z"
    }
   },
   "source": "from service.pipeline import ClassifierType",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading ticket-o3 from: /Users/giuliosacrestano/PycharmProjects/astromark-ai/service/../data/raw/ticket-o3.csv\n",
      "[INFO] Loaded ticket-o3.csv with shape (4027, 3)\n",
      "[INFO] Loading ticket-gemini from: /Users/giuliosacrestano/PycharmProjects/astromark-ai/service/../data/raw/ticket-gemini-claude.csv\n",
      "[INFO] Loaded ticket-gemini-claude.csv with shape (866, 3)\n",
      "[INFO] Merging dataframes...\n",
      "[INFO] Final merged dataset shape: (4893, 2)\n",
      "[INFO] Loading spaCy model (it_core_news_sm)...\n",
      "[INFO] Loading preprocessed data from '/Users/giuliosacrestano/PycharmProjects/astromark-ai/service/../data/processed/X_processed.csv'...\n",
      "[INFO] Selected classifier: svm\n",
      "[INFO] Retrieving final model (will load if already exists)...\n",
      "[INFO] No saved model found for svm. Training a new one...\n",
      "[INFO] Building pipeline for svm...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mservice\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ClassifierType\n",
      "File \u001B[0;32m~/PycharmProjects/astromark-ai/service/pipeline.py:228\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[INFO] Selected classifier: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mselected_classifier\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[INFO] Retrieving final model (will load if already exists)...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 228\u001B[0m final_model \u001B[38;5;241m=\u001B[39m \u001B[43mget_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mselected_classifier\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/astromark-ai/service/pipeline.py:217\u001B[0m, in \u001B[0;36mget_model\u001B[0;34m(classifier_type)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[INFO] No saved model found for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassifier_type\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Training a new one...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 217\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mperform_grid_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_processed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclassifier_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     save_model(model, classifier_type)\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/astromark-ai/service/pipeline.py:181\u001B[0m, in \u001B[0;36mperform_grid_search\u001B[0;34m(X, y, classifier_type)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mperform_grid_search\u001B[39m(X, y, classifier_type):\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[INFO] Building pipeline for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassifier_type\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 181\u001B[0m     pipeline, param_grid \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[INFO] Starting grid search for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassifier_type\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_grid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    183\u001B[0m     skf \u001B[38;5;241m=\u001B[39m StratifiedKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/astromark-ai/service/pipeline.py:164\u001B[0m, in \u001B[0;36mbuild_pipeline\u001B[0;34m(classifier_type)\u001B[0m\n\u001B[1;32m    162\u001B[0m     svd \u001B[38;5;241m=\u001B[39m TruncatedSVD(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m    163\u001B[0m     classifier \u001B[38;5;241m=\u001B[39m SVC(probability\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m--> 164\u001B[0m     pipeline \u001B[38;5;241m=\u001B[39m \u001B[43mPipeline\u001B[49m([\n\u001B[1;32m    165\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtfidf\u001B[39m\u001B[38;5;124m'\u001B[39m, tfidf),\n\u001B[1;32m    166\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msvd\u001B[39m\u001B[38;5;124m'\u001B[39m, svd),\n\u001B[1;32m    167\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclf\u001B[39m\u001B[38;5;124m'\u001B[39m, classifier)\n\u001B[1;32m    168\u001B[0m     ])\n\u001B[1;32m    169\u001B[0m     param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtfidf__min_df\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m],\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtfidf__max_df\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.85\u001B[39m, \u001B[38;5;241m0.90\u001B[39m],\n\u001B[1;32m    172\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msvd__n_components\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m50\u001B[39m],\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclf__C\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m1.0\u001B[39m]\n\u001B[1;32m    174\u001B[0m     }\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "0311fc80-0f71-4e6d-b82a-bba6e88b3ec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:33:58.205714Z",
     "start_time": "2025-02-05T11:33:58.198878Z"
    }
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Funzioni per costruire la pipeline e la Grid Search\n",
    "def build_pipeline(classifier_type):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        use_idf=True,\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=2000,\n",
    "        norm='l2',\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "\n",
    "    if classifier_type == ClassifierType.NAIVE_BAYES:\n",
    "        classifier = MultinomialNB()\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', tfidf),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "        param_grid = {\n",
    "            'tfidf__min_df': [1, 3],\n",
    "            'tfidf__max_df': [0.85, 0.90],\n",
    "            'clf__alpha': [1.0, 1.5, 2.0]\n",
    "        }\n",
    "    elif classifier_type == ClassifierType.SVM:\n",
    "        svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "        classifier = SVC(probability=True, kernel='linear', random_state=42)\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', tfidf),\n",
    "            ('svd', svd),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "        param_grid = {\n",
    "            'tfidf__min_df': [1, 3],\n",
    "            'tfidf__max_df': [0.85, 0.90],\n",
    "            'svd__n_components': [30, 50],\n",
    "            'clf__C': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier type.\")\n",
    "    return pipeline, param_grid\n",
    "\n",
    "def perform_grid_search(X, y, classifier_type):\n",
    "    print(f\"[INFO] Costruisco la pipeline per {classifier_type.value}...\")\n",
    "    pipeline, param_grid = build_pipeline(classifier_type)\n",
    "    print(f\"[INFO] Avvio Grid Search per {classifier_type.value} con parametri: {param_grid}\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=skf, n_jobs=-1, verbose=1)\n",
    "    grid.fit(X, y)\n",
    "    print(f\"[INFO] Grid Search per {classifier_type.value} completata.\")\n",
    "    print(f\"[INFO] Parametri migliori: {grid.best_params_}\")\n",
    "    return grid.best_estimator_\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "fd2892b2-8b9e-4f98-85d8-407e15b4b4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:34:03.856756Z",
     "start_time": "2025-02-05T11:34:03.849150Z"
    }
   },
   "source": [
    "# Funzioni per salvare/caricare il modello\n",
    "MODEL_PATHS = {\n",
    "    \"naive_bayes\": os.path.join(TRAINED_DIR, \"trained_model_nb.pkl\"),\n",
    "    \"svm\": os.path.join(TRAINED_DIR, \"trained_model_svm.pkl\")\n",
    "}\n",
    "\n",
    "def save_model(model, classifier_type):\n",
    "    path = MODEL_PATHS[classifier_type.value]\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"[INFO] Modello salvato in {path}.\")\n",
    "\n",
    "def load_model(classifier_type):\n",
    "    path = MODEL_PATHS[classifier_type.value]\n",
    "    if os.path.exists(path):\n",
    "        print(f\"[INFO] Caricamento del modello da {path}...\")\n",
    "        return joblib.load(path)\n",
    "    return None\n",
    "\n",
    "def get_model(classifier_type):\n",
    "    \"\"\"Se esiste un modello preaddestrato, lo carica; altrimenti esegue la grid search e lo salva.\"\"\"\n",
    "    model = load_model(classifier_type)\n",
    "    if model is None:\n",
    "        print(f\"[INFO] Nessun modello salvato per {classifier_type.value}. Avvio training...\")\n",
    "        model = perform_grid_search(X_processed, y, classifier_type)\n",
    "        save_model(model, classifier_type)\n",
    "    else:\n",
    "        print(f\"[INFO] Uso il modello salvato per {classifier_type.value}.\")\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "a65a49d5-3e3c-4c07-bdf3-22cdeb0d7e1b",
   "metadata": {},
   "source": [
    "## Fine della Pipeline\n",
    "\n",
    "I dati sono stati caricati, preprocessati e il modello (ottenuto tramite Grid Search) è stato caricato o addestrato e salvato nella cartella `../data/trained`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddf7b90e-5e0f-4d31-a8f9-0b2a43a9295e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:34:07.088661Z",
     "start_time": "2025-02-05T11:34:07.085298Z"
    }
   },
   "source": [
    "# Esempio: Funzione per il Ticket Service\n",
    "def ticket_service(title, message, classifier_type):\n",
    "    print(f\"[INFO] Classificatore selezionato: {classifier_type.value}\")\n",
    "    print(\"[INFO] Recupero del modello finale (lo carico se già esistente)...\")\n",
    "    get_model(classifier_type)\n",
    "    full_text = f\"{title} {message}\".strip()\n",
    "    if not full_text:\n",
    "        return [\"Nessun input fornito\"]\n",
    "    # Importa la funzione predict_category dal modulo report_predict\n",
    "    from service.report_predict import predict_category\n",
    "    predictions = predict_category(full_text, classifier_type)\n",
    "    print(\"Predizioni:\", predictions)\n",
    "    return predictions\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Esempio di utilizzo con SVM",
   "id": "340f44e50b86e240"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:34:10.757489Z",
     "start_time": "2025-02-05T11:34:10.727865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "example_title = \"Problema con la stampante\"\n",
    "example_message = \"La stampante non funziona correttamente.\"\n",
    "ticket_service(example_title, example_message, ClassifierType.SVM)"
   ],
   "id": "4264fba454ca7217",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassifierType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m example_title \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProblema con la stampante\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m example_message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLa stampante non funziona correttamente.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m ticket_service(example_title, example_message, \u001B[43mClassifierType\u001B[49m\u001B[38;5;241m.\u001B[39mSVM)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ClassifierType' is not defined"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Esempio di utilizzo con NAIVE BAYES",
   "id": "4344690adf3adaf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:34:15.129018Z",
     "start_time": "2025-02-05T11:34:15.107933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_title = \"Problema con la stampante\"\n",
    "example_message = \"La stampante non funziona correttamente.\"\n",
    "ticket_service(example_title, example_message, ClassifierType.NAIVE_BAYES)"
   ],
   "id": "53bc31ec1041948f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassifierType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m example_title \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProblema con la stampante\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m example_message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLa stampante non funziona correttamente.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m ticket_service(example_title, example_message, \u001B[43mClassifierType\u001B[49m\u001B[38;5;241m.\u001B[39mNAIVE_BAYES)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ClassifierType' is not defined"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
